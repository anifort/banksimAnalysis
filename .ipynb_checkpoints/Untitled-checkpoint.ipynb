{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 85%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 95%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from random import shuffle\n",
    "import time\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import tree, metrics, preprocessing\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "import graphviz, pydot\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer,accuracy_score, roc_auc_score, roc_curve, auc\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "from scipy import interp\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "    \n",
    "    \n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 85%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 95%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-181fb92d9dc4>, line 142)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-181fb92d9dc4>\"\u001b[0;36m, line \u001b[0;32m142\u001b[0m\n\u001b[0;31m    global cv=8\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class PDF(object):\n",
    "  def __init__(self, pdf, size=(200,200)):\n",
    "    self.pdf = pdf\n",
    "    self.size = size\n",
    "\n",
    "  def _repr_html_(self):\n",
    "    return '<iframe src={0}  onload=\"this.width=screen.width;this.height=screen.height/3;\"></iframe>'.format(self.pdf, self.size)\n",
    "\n",
    "  def _repr_latex_(self):\n",
    "    return r'\\includegraphics[width=1.0\\textwidth]{{{0}}}'.format(self.pdf)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "     \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    '''    \n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    '''\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot_cm(y,y_pred, class_names):\n",
    "    print('Confusion Matrix')\n",
    "    cnf_matrix = metrics.confusion_matrix(y, y_pred)\n",
    "    \n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                          title='Without normalization')\n",
    "    \n",
    "    # Plot normalized confusion matrix\n",
    "    # plt.figure()\n",
    "    # plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "    #                      title='Normalized')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def export_tree(dt, filename):\n",
    "    dot_data = tree.export_graphviz(dt, out_file=None,\n",
    "                         feature_names=df2.columns.values,  \n",
    "                         class_names=class_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)\n",
    "    graph = graphviz.Source(dot_data) \n",
    "    output = graph.render(filename) \n",
    "    print(\"results saved: \", output)\n",
    "    return output\n",
    "    # (graph,) = pydot.graph_from_dot_file(filename)\n",
    "    # graph.write_png(filename+'.png')\n",
    "    \n",
    "\n",
    "\n",
    "def plot_score_3d(results):\n",
    "    fig = plt.figure(figsize=(13, 13))\n",
    "    \n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # For each set of style and range settings, plot n random points in the box\n",
    "    # defined by x in [23, 32], y in [0, 100], z in [zlow, zhigh].\n",
    "    max_depth = np.array(results['param_max_depth'].data, dtype=float)\n",
    "    min_samples_leaf = np.array(results['param_min_samples_leaf'].data, dtype=float)\n",
    "    score = results['mean_test_Accuracy']\n",
    "    ax.scatter(max_depth, min_samples_leaf, score)\n",
    "\n",
    "    ax.set_xlabel('max_depth')\n",
    "    ax.set_ylabel('min_samples_leaf')\n",
    "    ax.set_zlabel('Score')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_score(results):\n",
    "    plt.figure(figsize=(13, 13))\n",
    "    plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",\n",
    "              fontsize=16)\n",
    "\n",
    "    plt.xlabel(\"max depth\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.grid()\n",
    "\n",
    "    ax = plt.axes()\n",
    "    #ax.set_xlim(0, 402)\n",
    "    ax.set_ylim(0.85, 1)\n",
    "\n",
    "    # Get the regular numpy array from the MaskedArray\n",
    "    X_axis = np.array(results['param_max_depth'].data, dtype=float)\n",
    "\n",
    "    for scorer, color in zip(sorted(scoring), ['g', 'k']):\n",
    "        for sample, style in (('train', '--'), ('test', '-')):\n",
    "            sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n",
    "            sample_score_std = results['std_%s_%s' % (sample, scorer)]\n",
    "            ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
    "                            sample_score_mean + sample_score_std,\n",
    "                            alpha=0.1 if sample == 'test' else 0, color=color)\n",
    "            ax.plot(X_axis, sample_score_mean, style, color=color,\n",
    "                    alpha=1 if sample == 'test' else 0.7,\n",
    "                    label=\"%s (%s)\" % (scorer, sample))\n",
    "\n",
    "        best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "        best_score = results['mean_test_%s' % scorer][best_index]\n",
    "\n",
    "        # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "        ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
    "                linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
    "\n",
    "        # Annotate the best score for that scorer\n",
    "        ax.annotate(\"%0.2f\" % best_score,\n",
    "                    (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid('off')\n",
    "    plt.show()\n",
    "    \n",
    "def report(y,y_pred,class_names):\n",
    "    plot_cm(y,y_pred,class_names)\n",
    "    print(metrics.classification_report(y, y_pred))\n",
    "    print('ROC_AUC score', roc_auc_score(y,y_pred))\n",
    "    \n",
    "def reset_data():\n",
    "    global cv=8\n",
    "    global class_names = ['not fraud', 'fraud']\n",
    "    global dt = DecisionTreeClassifier()\n",
    "    \n",
    "    df = pd.read_csv('bs140513_032310.csv')\n",
    "    df.age=df.age.map(lambda x: x.lstrip(\"'\").rstrip(\"'\"))\n",
    "    df.merchant=df.merchant.map(lambda x: x.lstrip(\"'\").rstrip(\"'\"))\n",
    "    df.customer=df.customer.map(lambda x: x.lstrip(\"'\").rstrip(\"'\"))\n",
    "    df.category=df.category.map(lambda x: x.lstrip(\"'\").rstrip(\"'\"))\n",
    "    \n",
    "    df['age']= df['age'].str.replace('U','-1')\n",
    "    df.age = pd.to_numeric(df.age)\n",
    "    \n",
    "    df = df.drop(['zipcodeOri', 'zipMerchant'], axis=1)\n",
    "    \n",
    "    df.loc[df.gender == \"'M'\", 'gender'] = 0\n",
    "    df.loc[df.gender == \"'F'\", 'gender'] = 1\n",
    "    df.loc[df.gender == \"'E'\", 'gender'] = -1\n",
    "    df.loc[df.gender == \"'U'\", 'gender'] = -1\n",
    "    df.gender = pd.to_numeric(df.gender) # ensure there are no str left\n",
    "    \n",
    "    df.merchant=df.merchant.map(lambda x: x.lstrip(\"M\"))\n",
    "    df.customer=df.customer.map(lambda x: x.lstrip(\"C\"))\n",
    "\n",
    "    df.customer = pd.to_numeric(df.customer)\n",
    "    df.merchant = pd.to_numeric(df.merchant)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reset_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d74727eae923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reset_data' is not defined"
     ]
    }
   ],
   "source": [
    "df = reset_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model without parameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df\n",
    "y = df2[\"fraud\"]\n",
    "df2 = df2.drop([\"fraud\", 'customer', 'step'], axis=1)\n",
    "df2=pd.get_dummies(df2, columns=[\"category\"])\n",
    "\n",
    "X = df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model with 10 cross validations and default parameters, in all cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X, y)\n",
    "y_pred_default = cross_val_predict(dt, X, y, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y, y_pred_default))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_cm(y,y_pred_default,class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export tree graph in pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = export_tree(dt,'dt_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PDF(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model with parameter tunning (grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        'class_weight':[None],\n",
    "        'criterion':['entropy'],\n",
    "        'max_depth':range(2,40, 4),\n",
    "        #'max_depth':range(13,16, 1),\n",
    "        'min_samples_leaf':range(12, 14, 4),\n",
    "        'max_features':[None],\n",
    "        'max_leaf_nodes':[None],\n",
    "        'min_impurity_decrease':[0.0],\n",
    "        'min_impurity_split':[None],\n",
    "        'min_samples_split':[2],\n",
    "        'min_weight_fraction_leaf':[0.0],\n",
    "        'presort':[False],\n",
    "        'random_state':[87],\n",
    "        'splitter':['best']\n",
    "}\n",
    "\n",
    "scoring = {'AUC': make_scorer(roc_auc_score), 'Accuracy': make_scorer(accuracy_score)}\n",
    "gs = GridSearchCV(dt,n_jobs=-1, \n",
    "                  param_grid=params, \n",
    "                  scoring=scoring, \n",
    "                  return_train_score=True, \n",
    "                  cv=cv, \n",
    "                  refit='Accuracy',verbose=1)\n",
    "\n",
    "start_time = time.time()\n",
    "gs.fit(X, y)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"ellapsed time\", elapsed_time)\n",
    "#gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(gs.best_params_)\n",
    "#print(\"Grid scores on development set:\")\n",
    "#print()\n",
    "#means = gs.cv_results_['mean_test_AUC']\n",
    "#stds = gs.cv_results_['std_test_AUC']\n",
    "#for mean, std, params in zip(means, stds, gs.cv_results_['params']):\n",
    "#    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "#          % (mean, std * 2, params))\n",
    "#print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC score: \\t\",gs.cv_results_['mean_test_AUC'].mean())\n",
    "print(\"Accuracy score:\\t\",gs.cv_results_['mean_test_Accuracy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model with optimal parameters,10 cross validations and default parameters, in all cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_param_tune = cross_val_predict(dt, X, y, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(y,y_pred_param_tune,class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export tree graph in pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = export_tree(dt,'dt_param_tune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PDF(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering : create additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reset_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cust_tl_count_trans'] = df['amount'].groupby(df['customer']).transform('count')\n",
    "#df['cust_cat_count_trans'] = df.groupby(['customer', 'category'])['amount'].transform('count')\n",
    "#df['cust_mer_count_trans'] = df.groupby(['customer', 'merchant'])['amount'].transform('count')\n",
    "\n",
    "df['cust_tl_mean_amount'] = df['amount'].groupby(df['customer']).transform('mean')\n",
    "df['cat_mean_amount'] = df.groupby(['category'])['amount'].transform('mean')\n",
    "df['cust_mer_mean_amount'] = df.groupby(['merchant'])['amount'].transform('mean')\n",
    "#df['cust_cat_mean_amount'] = df.groupby(['customer', 'category'])['amount'].transform('mean')\n",
    "#df['cust_mer_mean_amount'] = df.groupby(['customer', 'merchant'])['amount'].transform('mean')\n",
    "\n",
    "df['cust_tl_median_amount'] = df['amount'].groupby(df['customer']).transform('median')\n",
    "#df['cust_cat_median_amount'] = df.groupby(['customer', 'category'])['amount'].transform('median')\n",
    "#df['cust_mer_median_amount'] = df.groupby(['customer', 'category'])['amount'].transform('median')\n",
    "\n",
    "df['cust_tl_std_amount'] = df['amount'].groupby(df['customer']).transform('std')\n",
    "df['cat_mean_amount'] = df.groupby(['category'])['amount'].transform('std')\n",
    "df['cust_mer_mean_amount'] = df.groupby(['merchant'])['amount'].transform('std')\n",
    "#df['cust_cat_std_amount'] = df.groupby(['customer', 'category'])['amount'].transform('std')\n",
    "#df['cust_mer_std_amount'] = df.groupby(['customer', 'merchant'])['amount'].transform('std')\n",
    "\n",
    "df['cust_tl_max_amount'] = df['amount'].groupby(df['customer']).transform('max')\n",
    "df['cat_mean_amount'] = df.groupby(['category'])['amount'].transform('max')\n",
    "df['cust_mer_mean_amount'] = df.groupby(['merchant'])['amount'].transform('max')\n",
    "#df['cust_cat_max_amount'] = df.groupby(['customer', 'category'])['amount'].transform('max')\n",
    "#df['cust_mer_max_amount'] = df.groupby(['customer', 'merchant'])['amount'].transform('max')\n",
    "\n",
    "df['cust_tl_min_amount'] = df['amount'].groupby(df['customer']).transform('min')\n",
    "df['cat_mean_amount'] = df.groupby(['category'])['amount'].transform('min')\n",
    "df['cust_mer_mean_amount'] = df.groupby(['merchant'])['amount'].transform('min')\n",
    "#df['cust_cat_min_amount'] = df.groupby(['customer', 'category'])['amount'].transform('min')\n",
    "#df['cust_mer_min_amount'] = df.groupby(['customer', 'merchant'])['amount'].transform('min')\n",
    "\n",
    "df = df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# the size of A4 paper\n",
    "fig.set_size_inches(35, 12)\n",
    "ax = sns.boxplot(x=\"category\", y=\"amount\", data=df[df['category']!='es_travel'], palette=\"Set3\")\n",
    "ax.set_ylim(-1,1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_sample = list(set(list(df['category'])))\n",
    "customer_sample = list(set(list(df['customer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# the size of A4 paper\n",
    "fig.set_size_inches(35, 12)\n",
    "ax = sns.boxplot(x=\"customer\", y=\"amount\", data=df[df['customer'].isin(customer_sample[:14])], palette=\"Set3\")\n",
    "ax.set_ylim(-1,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "clf = LocalOutlierFactor()\n",
    "def outlier_detect(amount, neigh):\n",
    "    if (len(amount)>neigh):\n",
    "        clf.n_neighbors = neigh\n",
    "    elif (len(amount)==1):\n",
    "        return [1]\n",
    "    else:\n",
    "        clf.n_neighbors = len(amount)-1\n",
    "\n",
    "    yy = clf.fit_predict(np.array(amount).reshape(-1, 1))\n",
    "    return yy\n",
    "    \n",
    "\n",
    "df['ol_cust_cat_amount'] = df.groupby(['customer','category'])['amount'].transform(outlier_detect,30)\n",
    "df['ol_cust_mer_amount'] = df.groupby(['customer','merchant'])['amount'].transform(outlier_detect,30)\n",
    "df['ol_cust_tl_amount'] = df.groupby(['customer'])['amount'].transform(outlier_detect,30)\n",
    "df['ol_cat_amount'] = df.groupby(['category'])['amount'].transform(outlier_detect,30)\n",
    "df['ol_mer_amount'] = df.groupby(['merchant'])['amount'].transform(outlier_detect,30)\n",
    "'''\n",
    "\n",
    "def outlier_detect_boxplot(amount):\n",
    "    median = np.median(amount)\n",
    "    std = np.std(amount)\n",
    "    upper_quartile = np.percentile(amount, 77) #75\n",
    "    lower_quartile = np.percentile(amount, 23) #25\n",
    "\n",
    "    iqr = upper_quartile - lower_quartile\n",
    "    upper_whisker = amount[amount<=upper_quartile+1.5*iqr].max()\n",
    "    lower_whisker = amount[amount>=lower_quartile-1.5*iqr].min()\n",
    "    return [0 if (x<upper_whisker and x>lower_whisker) else 1 for x in amount]\n",
    "   \n",
    "\n",
    "df['ol_cust_cat_amount'] = df.groupby(['customer','category'])['amount'].transform(outlier_detect_boxplot)\n",
    "df['ol_cust_mer_amount'] = df.groupby(['customer','merchant'])['amount'].transform(outlier_detect_boxplot)\n",
    "df['ol_cust_tl_amount'] = df.groupby(['customer'])['amount'].transform(outlier_detect_boxplot)\n",
    "df['ol_cat_amount'] = df.groupby(['category'])['amount'].transform(outlier_detect_boxplot)\n",
    "df['ol_mer_amount'] = df.groupby(['merchant'])['amount'].transform(outlier_detect_boxplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['fraud','ol_cust_cat_amount','ol_cust_mer_amount','ol_cust_tl_amount','ol_cat_amount','ol_mer_amount']].corr()\n",
    "#df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fraud in existing data per category')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(35, 12)\n",
    "#ax.set_yticks([-1,0,1,2])\n",
    "ax = sns.stripplot('category', 'fraud', data=df, jitter=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Outliers based on category spending amount overall (per category)')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(35, 12)\n",
    "ax = sns.stripplot(x=\"category\", y=\"amount\", data=df[df['category']!='es_travel'], hue=\"ol_cat_amount\", jitter=True, palette=\"Set1\");\n",
    "ax.set_ylim(-1,1410)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip1= sns.lmplot(x=\"step\", y=\"amount\", hue=\"ol_cust_tl_amount\", col=\"customer\", data=df[df['customer'].isin(customer_sample[:4])], aspect=1.4, x_jitter=0, fit_reg=False, palette=\"Set1\")\n",
    "ip2= sns.lmplot(x=\"step\", y=\"amount\", hue=\"ol_cust_tl_amount\", col=\"customer\", data=df[df['customer'].isin(customer_sample[4:8])], aspect=1.4, x_jitter=0, fit_reg=False, palette=\"Set1\")\n",
    "ip3= sns.lmplot(x=\"step\", y=\"amount\", hue=\"ol_cust_tl_amount\", col=\"customer\", data=df[df['customer'].isin(customer_sample[8:12])], aspect=1.4, x_jitter=0, fit_reg=False, palette=\"Set1\")\n",
    "\n",
    "axes = ip1.axes\n",
    "axes[0,0].set_ylim(-10,700)\n",
    "axes = ip2.axes\n",
    "axes[0,0].set_ylim(-10,700)\n",
    "axes = ip3.axes\n",
    "axes[0,0].set_ylim(-10,700)\n",
    "print('Outliers based on customer spending amount overall (per customer)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df\n",
    "#df2=df.sample(frac=1)\n",
    "y = df2[\"fraud\"]\n",
    "''' \n",
    "# keep original values before standartization\n",
    "keep_orig_columns = ['ol_cust_cat_amount','ol_cust_mer_amount','ol_cust_tl_amount','ol_cat_amount','ol_mer_amount']\n",
    "keep_as_is_df = df2[keep_orig_columns]\n",
    "\n",
    "cat_df=pd.get_dummies(df2[\"category\"], columns=[\"category\"])\n",
    "\n",
    "df2 = df2.drop([\"fraud\", 'customer', 'step', 'gender', 'category'], axis=1)\n",
    "df2 = df2.drop(keep_orig_columns, axis=1)\n",
    "\n",
    "scaled_features = preprocessing.StandardScaler().fit_transform(df2.values)\n",
    "df2 = pd.DataFrame(scaled_features, index=df2.index, columns=df2.columns)\n",
    "\n",
    "X = pd.concat([df2,cat_df,keep_as_is_df], axis=1)\n",
    "'''\n",
    "\n",
    "\n",
    "cat_df=pd.get_dummies(df2[\"category\"], columns=[\"category\"])\n",
    "df2 = df2.drop([\"fraud\", 'customer', 'step', 'gender', 'category'], axis=1)\n",
    "scaled_features = preprocessing.StandardScaler().fit_transform(df2.values)\n",
    "df2 = pd.DataFrame(scaled_features, index=df2.index, columns=df2.columns)\n",
    "\n",
    "df2 = pd.concat([df2,cat_df], axis=1)\n",
    "\n",
    "X=df2\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        'class_weight':[None],\n",
    "        'criterion':['entropy'],\n",
    "        'max_depth':range(2,40, 4),\n",
    "        #'max_depth':range(13,16, 1),\n",
    "        'min_samples_leaf':range(12,14, 4),\n",
    "        'max_features':[None],\n",
    "        'max_leaf_nodes':[None],\n",
    "        'min_impurity_decrease':[0.0],\n",
    "        'min_impurity_split':[None],\n",
    "        'min_samples_split':[2],\n",
    "        'min_weight_fraction_leaf':[0.0],\n",
    "        'presort':[False],\n",
    "        'random_state':[87],\n",
    "        'splitter':['best']\n",
    "}\n",
    "\n",
    "scoring = {'AUC': make_scorer(roc_auc_score), 'Accuracy': make_scorer(accuracy_score)}\n",
    "gs = GridSearchCV(dt,n_jobs=-1, \n",
    "                  param_grid=params, \n",
    "                  scoring=scoring, \n",
    "                  return_train_score=True, \n",
    "                  cv=cv, \n",
    "                  refit='Accuracy',verbose=1)\n",
    "\n",
    "start_time = time.time()\n",
    "gs.fit(X, y)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"ellapsed time\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC score: \\t\",gs.cv_results_['mean_test_AUC'].mean())\n",
    "print(\"Accuracy score:\\t\",gs.cv_results_['mean_test_Accuracy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = gs.best_estimator_\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_fe_param_tune = cross_val_predict(dt, X, y, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report(y,y_pred_fe_param_tune,class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = export_tree(dt,'dt_param_tune_fe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PDF(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_fe_param_tune_prob = cross_val_predict(dt, X, y, cv=cv, n_jobs=-1, method='predict_proba')\n",
    "y_prob = [c[1] for c in y_pred_fe_param_tune_prob]\n",
    "\n",
    "df_tmp = pd.concat([df, pd.DataFrame(y_pred_fe_param_tune, columns = [\"prediction\"]), pd.DataFrame(y_prob, columns = [\"probability\"])], axis=1)\n",
    "df_tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data in ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from elasticsearch import helpers\n",
    "es = Elasticsearch()\n",
    "list_rec = df_tmp.to_json(orient='records')\n",
    "list_rec = json.loads(list_rec)\n",
    "\n",
    "count=0\n",
    "for rec in list_rec:\n",
    "    count+=1\n",
    "    try:\n",
    "        res = es.index(index=\"test-index\", doc_type='transaction', body=rec)\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import h2o\n",
    "#from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "\n",
    "#m = H2ODeepLearningEstimator()       \n",
    "#m.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "import numpy\n",
    "\n",
    "class EarlyStoppingByLossVal(Callback):\n",
    "    def __init__(self, monitor='val_loss', value=0.00001, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current < self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "\n",
    "            \n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(15, input_dim=len(X.columns.values), kernel_initializer=init, activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "seed = 7\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "#---------------------------------------------\n",
    "#optimizers = ['rmsprop', 'adam']\n",
    "#init = ['glorot_uniform', 'normal', 'uniform']\n",
    "#epochs = [50, 100, 150]\n",
    "kfold_weights_path='fw.h5'\n",
    "callbacks = [\n",
    "    EarlyStoppingByLossVal(monitor='val_loss', value=0.00001, verbose=1),\n",
    "    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "    ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "]\n",
    "\n",
    "\n",
    "history = model.fit(X[:-15000], y[:-15000], batch_size = 64, \n",
    "          epochs = 100, validation_split = 0.25, #callbacks=callbacks\n",
    "                   )\n",
    "\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X[:-15000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['acc'][-10:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dl = model.predict(X[-15000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(y[-15000:],y_pred_dl,class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
